# https://gitlab.com/cscs-ci/ci-testing/webhook-ci/gitlab-runner-k8s-container-builder.git
include:
  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml'

stages:
  - SPHbase     # no srun (extends=.container-builder-dynamic-name)
  - SPHbuild    # no srun (extends=.container-builder)
  - SPHpull     # on daint
  - SPHtest     # on daint

#{{{ variables
variables:
  # version-tag will be added by .container-builder-dynamic-name, see https://gitlab.com/cscs-ci/ci-testing/containerised_ci_doc/-/blob/main/dependency_management.md
  BASE_CUDA: "${CSCS_REGISTRY_PATH}/base/sph-exa_base-cuda"
  BASE_HIP: "${CSCS_REGISTRY_PATH}/base/sph-exa_base-hip"
  BUILD_CUDA: "${CSCS_REGISTRY_PATH}/sph-exa_build:cuda-$CI_COMMIT_SHORT_SHA"
  # BUILD_CUDA1: "${CSCS_REGISTRY_PATH}/sph-exa_build:1.0"
  BUILD_HIP: "${CSCS_REGISTRY_PATH}/sph-exa_build:hip-$CI_COMMIT_SHORT_SHA"
  DEPS_PATH: 'ftp://ftp.cscs.ch/out/jgp/hpc/containers'
  VERBOSE: 'YES'
  #no: SYSTEM_NAME: '.dom'
  # REBUILD_BASE_IMAGE: 'YES'
  # SARUS_VERBOSE: 'YES'
#}}}

#{{{ sph:base:cuda:
sph:base:cuda:
  extends: .container-builder-dynamic-name
  stage: SPHbase
  variables:
    # tried with if/else, no success -> must comment/uncomment
    # ---
    # DOCKERFILE: '.gitlab/Dockerfile_1'
    # PERSIST_IMAGE_NAME: "${BASE_CUDA}"
    # ---
    DOCKERFILE: '.gitlab/Dockerfile_1'
    PERSIST_IMAGE_NAME: "${BASE_CUDA}"
    WATCH_FILECHANGES: '.gitlab/Dockerfile_1'

#}}}
#{{{ sph:base:hip:
sph:base:hip:
  extends: .container-builder-dynamic-name
  stage: SPHbase
  variables:
    # tried with if/else, no success -> must comment/uncomment
    # ---
    # DOCKERFILE: '.gitlab/Dockerfile_hip_1'
    # PERSIST_IMAGE_NAME: "${BASE_HIP}"
    # ---
    DOCKERFILE: '.gitlab/Dockerfile_hip_1'
    PERSIST_IMAGE_NAME: "${BASE_HIP}"
    WATCH_FILECHANGES: '.gitlab/Dockerfile_hip_1'
#}}}

#{{{ sph:build:cuda:
sph:build:cuda:
  needs: ['sph:base:cuda']
  extends: .container-builder
  stage: SPHbuild
  before_script:
    - echo "PERSIST_IMAGE_NAME=$PERSIST_IMAGE_NAME"
  variables:
    # JG: 'YES'
    DOCKERFILE: '.gitlab/Dockerfile_2'
    PERSIST_IMAGE_NAME: "${BUILD_CUDA}"
    DOCKER_BUILD_ARGS: '["BASE_IMAGE=$BASE_IMAGE"]'
#}}}
#{{{ sph:build:hip:
sph:build:hip:
  needs: ['sph:base:hip']
  extends: .container-builder
  stage: SPHbuild
  before_script:
    - echo "PERSIST_IMAGE_NAME=$PERSIST_IMAGE_NAME"
  variables:
    DOCKERFILE: '.gitlab/Dockerfile_hip_2'
    PERSIST_IMAGE_NAME: "${BUILD_HIP}"
    DOCKER_BUILD_ARGS: '["BASE_IMAGE=$BASE_IMAGE"]'
#}}}

#{{{ sph:pull:cuda:
sph:pull:cuda:
  needs: ['sph:build:cuda']
  extends: .container-runner-daint-gpu
  stage: SPHpull
  image: ${BUILD_CUDA}
  script:
    - echo "Pulling image=${BUILD_CUDA}"
    - echo "  CSCS_REGISTRY_PATH=${CSCS_REGISTRY_PATH}"
    - echo "  PERSIST_IMAGE_NAME=${PERSIST_IMAGE_NAME}"
  variables:
    PULL_IMAGE: 'YES'
    PERSIST_IMAGE_NAME: "${BUILD_CUDA}"
#}}}

#{{{ sph:test:cuda:1:
sph:build:cuda:1:
  needs: ['sph:pull:cuda']
  extends: .container-runner-daint-gpu
  stage: SPHtest
  image: ${BUILD_CUDA}
  script:
    - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
    - ls -la /usr/local/games/
    - ln -fs /usr/local/sbin/hydro/example_data.txt example_data.txt
    - /usr/local/sbin/coord_samples/coordinate_test
    - /usr/local/sbin/performance/scan_perf
    - /usr/local/sbin/coord_samples/coordinate_test
    - /usr/local/sbin/hydro/kernel_tests_ve
    - /usr/local/sbin/hydro/turbulence_tests  
    - /usr/local/sbin/hydro/kernel_tests_std
    - /usr/local/sbin/ryoanji/cpu_unit_tests/ryoanji_cpu_unit_tests
    - /usr/local/sbin/unit/component_units
    - /usr/local/sbin/unit/component_units_omp
    - /usr/local/sbin/performance/peers_perf
  variables:
    USE_MPI: 'NO'
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    PULL_IMAGE: 'NO'
    # SLURM_PARTITION
    # SLURM_TIMELIMIT    
#}}}
#{{{ sph:test:cuda:2:
sph:build:cuda:2:
  needs: ['sph:pull:cuda']
  extends: .container-runner-daint-gpu
  stage: SPHtest
  image: ${BUILD_CUDA}
  script:
    - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
    - /usr/local/sbin/integration_mpi/domain_2ranks
    - /usr/local/sbin/integration_mpi/exchange_focus
    - /usr/local/sbin/integration_mpi/exchange_halos
    - /usr/local/sbin/integration_mpi/focus_transfer
    - /usr/local/sbin/integration_mpi/globaloctree
    # - /usr/local/sbin/integration_mpi/exchange_halos_gpu # needs 2 gpus
  variables:
    USE_MPI: 'YES'
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 2
    PULL_IMAGE: 'NO'
    # SLURM_PARTITION
    # SLURM_TIMELIMIT    
#}}}
#{{{ sph:test:cuda:2cn:
# sph:build:cuda:2cn:
#   needs: ['sph:pull:cuda']
#   extends: .container-runner-daint-gpu
#   stage: SPHtest
#   image: ${BUILD_CUDA}
#   script:
#     - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
#     - /usr/local/sbin/integration_mpi/assignment_gpu
#     - /usr/local/sbin/integration_mpi/domain_gpu
#     - /usr/local/sbin/integration_mpi/exchange_domain_gpu
#   variables:
#     USE_MPI: 'YES'
#     SLURM_JOB_NUM_NODES: 2
#     SLURM_NTASKS: 2
#     PULL_IMAGE: 'NO'
#     SLURM_PARTITION: 'debug'
#     # SLURM_TIMELIMIT
#}}}
#{{{ sph:test:cuda:5:
sph:build:cuda:5:
  needs: ['sph:pull:cuda']
  extends: .container-runner-daint-gpu
  stage: SPHtest
  image: ${BUILD_CUDA}
  script:
    - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
    - /usr/local/sbin/integration_mpi/exchange_domain
    - /usr/local/sbin/integration_mpi/box_mpi
    - /usr/local/sbin/integration_mpi/focus_tree
    - /usr/local/sbin/integration_mpi/treedomain
    - /usr/local/sbin/integration_mpi/domain_nranks
    - /usr/local/sbin/integration_mpi/exchange_general
    - /usr/local/sbin/integration_mpi/exchange_keys
    - /usr/local/sbin/ryoanji/global_upsweep_cpu
  variables:
    USE_MPI: 'YES'
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 5
    PULL_IMAGE: 'NO'
    # SLURM_PARTITION
    # SLURM_TIMELIMIT    
#}}}
#{{{ sph:test:cuda:p100:
sph:build:cuda:p100:
  needs: ['sph:pull:cuda']
  extends: .container-runner-daint-gpu
  stage: SPHtest
  image: ${BUILD_CUDA}
  script:
    - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
    - /usr/local/sbin/performance/hilbert_perf_gpu
    # - /usr/local/sbin/performance/cudaNeighborsTest
    # - /usr/local/sbin/performance/neighbors_test_gpu
    # - /usr/local/sbin/performance/exchange_halos_gpu # moved to integration_mpi
    - /usr/local/sbin/performance/octree_perf_gpu
    - /usr/local/sbin/unit_cuda/component_units_cuda
    - /usr/local/sbin/ryoanji/unit_tests/ryoanji_unit_tests
    - /usr/local/sbin/ryoanji/global_upsweep_gpu
    - /usr/local/sbin/ryoanji/ryoanji_demo/ryoanji_demo 
  variables:
    USE_MPI: 'NO'
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    PULL_IMAGE: 'NO'
    # SLURM_PARTITION
    # SLURM_TIMELIMIT    
#}}}
#{{{ sph:test:cuda:sphexa:cpu:
sph:build:cuda:sphexa:cpu:
  needs: ['sph:pull:cuda']
  extends: .container-runner-daint-gpu
  stage: SPHtest
  image: ${BUILD_CUDA}
  script:
    - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
    - export LD_LIBRARY_PATH=/usr/local/HDF_Group/HDF5/1.13.2/lib:$LD_LIBRARY_PATH
    - wget --quiet ${DEPS_PATH}/in/glass.h5
    - echo "sedov:cpu"
    - /usr/local/bin/sphexa --init sedov --prop std -s 1 -n 50
    - echo "sedov+ve:cpu"
    - /usr/local/bin/sphexa --init sedov --prop ve -s 1 -n 50
    - echo "noh:cpu"
    - /usr/local/bin/sphexa --init noh --glass ./glass.h5 -s 1 -n 50
  variables:
    USE_MPI: 'YES'
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    PULL_IMAGE: 'NO'
    # SLURM_PARTITION
    # SLURM_TIMELIMIT    
#}}}
#{{{ sph:test:cuda:sphexa:gpu:
# TODO: MPICH_RDMA_ENABLED_CUDA=1 LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libcuda.so
sph:build:cuda:sphexa:gpu:
  needs: ['sph:pull:cuda' ]
  extends: .container-runner-daint-gpu
  stage: SPHtest
  image: ${BUILD_CUDA}
  script:
    - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
    - export LD_LIBRARY_PATH=/usr/local/HDF_Group/HDF5/1.13.2/lib:$LD_LIBRARY_PATH
    - ln -fs /usr/local/bin/sedov_solution .
    - wget --quiet ${DEPS_PATH}/in/glass.h5
    - echo "# --- sedov:gpu"
    - /usr/local/bin/sphexa-cuda --init sedov -s 200 -n 50 -w 200 -f x,y,z,h,m,p,rho,vx,vy,vz,temp -o /scratch/out_sedov.h5 --quiet
    - python3 /usr/local/bin/compare_solutions.py -s 200 /scratch/out_sedov.h5 > /scratch/sedov.rpt
    #
    - echo "# --- noh:gpu"
    - /usr/local/bin/sphexa-cuda --init noh --glass ./glass.h5 -s 200 -n 50 -w 200 -f x,y,z,h,m,p,rho,vx,vy,vz,temp -o /scratch/out_noh.h5 --quiet
    - python3 /usr/local/bin/compare_noh.py -s 200 /scratch/out_noh.h5 > /scratch/noh.rpt
    - echo "# --- evrard:gpu"
    - /usr/local/bin/sphexa-cuda --init evrard --glass ./glass.h5 -s 10 -n 50 -w 10 --outDir /scratch/ --quiet
    - echo "# --- rpt:"
    - cat /scratch/sedov.rpt
    - cat /scratch/noh.rpt
    - pwd
    - ls -la
    - reframe -c /usr/local/games/rfm.py -r -S rpt_path=/scratch
  variables:
    USE_MPI: 'YES'
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    PULL_IMAGE: 'NO'
    # SLURM_PARTITION
    # SLURM_TIMELIMIT    
#}}}
